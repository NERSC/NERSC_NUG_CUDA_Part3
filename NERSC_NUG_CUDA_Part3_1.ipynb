{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67de134",
   "metadata": {},
   "source": [
    "![NERSC Logo](NERSClogo087.png)\n",
    "# NERSC Hands-On: Introduction to CUDA C/C++\n",
    "## Part 3 Notbook 1: Understanding Kernels, Grids, and Blocks\n",
    "\n",
    "Welcome to the NERSC introduction to CUDA C/C++! This 1-hour session is designed to run on Perlmutter via the NERSC JupyterHub.\n",
    "\n",
    "**Goal:** Move from zero to writing, compiling, and running your first simple CUDA C++ programs. We will focus on the fundamental concepts of the CUDA programming model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea73bb",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "* **Verify** your GPU environment on Perlmutter.\n",
    "* **Define** the core CUDA concepts: Kernel, Thread, Block, and Grid.\n",
    "* **Write** a \"Hello, World!\" kernel in CUDA C++ using `%%writefile`.\n",
    "* **Compile** CUDA C++ code using the NVIDIA compiler (`nvcc`).\n",
    "* **Launch** a kernel and see parallel execution in action.\n",
    "* **Implement** a simple, data-parallel kernel for vector addition.\n",
    "* **Understand** the difference between Host (CPU) and Device (GPU) memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64902b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Setup: Verify Your Environment\n",
    "\n",
    "Let's run two commands:\n",
    "1.  `module load cudatoolkit`: Loads the NVIDIA compiler and libraries.\n",
    "2.  `nvidia-smi`: The NVIDIA System Management Interface. This command shows us what GPU(s) are attached and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load cudatoolkit\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a67b0",
   "metadata": {},
   "source": [
    "> ‚ö™Ô∏è **REFLECTION:** You should see a table listing one or more NVIDIA GPUs (e.g., \"NVIDIA A100-SXM4-80GB\"). The output from `nvidia-smi` confirms the GPU is visible to your session. The `module load` command makes the `nvcc` compiler available, which we will use next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea6b9f",
   "metadata": {},
   "source": [
    "## 2. Core Concept: The CUDA Execution Model\n",
    "\n",
    "How does a GPU execute code? It follows a specific hierarchy.\n",
    "\n",
    "1.  **Host (CPU) vs. Device (GPU):**\n",
    "    * Your normal C++ `main()` function runs on the **Host** (the CPU).\n",
    "    * Your parallel functions run on the **Device** (the GPU).\n",
    "\n",
    "2.  **Kernel:**\n",
    "    * A C/C++ function that runs on the GPU. You write it once, and it is executed by *many* threads in parallel.\n",
    "    * You define a kernel using the `__global__` keyword.\n",
    "\n",
    "3.  **Thread, Block, and Grid:**\n",
    "    * **Thread:** The smallest unit of execution. It runs one copy of your kernel.\n",
    "    * **Block:** A group of threads. Threads *within* a block can cooperate using fast **Shared Memory**.\n",
    "    * **Grid:** A group of blocks. This is the full set of threads that you launch to run your kernel.\n",
    "\n",
    "\n",
    "\n",
    "When you launch a kernel, you tell CUDA how many blocks to launch, and how many threads to put in each block:\n",
    "\n",
    "```cpp\n",
    "// C++ syntax for launching a kernel\n",
    "int numBlocks = 64;\n",
    "int threadsPerBlock = 256;\n",
    "myKernel<<<numBlocks, threadsPerBlock>>>(...arguments...);\n",
    "```\n",
    "\n",
    "Inside the kernel, each thread has built-in variables to know *who* it is and *where* it is:\n",
    "* `threadIdx.x`: Your thread ID *within* the block.\n",
    "* `blockIdx.x`: Your block ID *within* the grid.\n",
    "* `blockDim.x`: The total number of threads in your block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd37a2",
   "metadata": {},
   "source": [
    "## 3. \"Hello, World!\" in CUDA C++\n",
    "\n",
    "Let's write our first kernel. We'll use the `%%writefile` \"magic command\" to create a new file named `hello.cu` right from this notebook cell. (`.cu` is the standard extension for CUDA C++ files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15678554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hello_cuda.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "/*\n",
    " * A CUDA Kernel: A function that runs on the GPU.\n",
    " * Defined by the __global__ keyword.\n",
    " */\n",
    "__global__ void hello_kernel() {\n",
    "    /*\n",
    "     * Calculate this thread's unique, global ID.\n",
    "     * This is the most common pattern in CUDA.\n",
    "     */\n",
    "    int blockID = blockIdx.x;\n",
    "    int threadID_in_block = threadIdx.x;\n",
    "    int threads_per_block = blockDim.x;\n",
    "    \n",
    "    int globalThreadID = blockID * threads_per_block + threadID_in_block;\n",
    "    \n",
    "    printf(\"Hello from global thread %d! (Block %d, Thread %d)\\n\", \n",
    "           globalThreadID, blockID, threadID_in_block);\n",
    "}\n",
    "\n",
    "/*\n",
    " * The main function runs on the Host (CPU).\n",
    " */\n",
    "int main() {\n",
    "    printf(\"--- Starting kernel from CPU ---\\n\\n\");\n",
    "\n",
    "    // Define the Grid and Block dimensions\n",
    "    int numBlocks = 2;         // The Grid will have 2 blocks\n",
    "    int threadsPerBlock = 8;   // Each block will have 8 threads\n",
    "\n",
    "    // Launch the kernel on the Device (GPU)\n",
    "    // Syntax: kernel_name<<<Grid, Block>>>(...arguments...)\n",
    "    hello_kernel<<<numBlocks, threadsPerBlock>>>();\n",
    "\n",
    "    /*\n",
    "     * Wait for the GPU to finish before the CPU continues.\n",
    "     * This is CRUCIAL. Without it, the CPU's main() might exit \n",
    "     * before the GPU kernel even starts!\n",
    "     */\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    printf(\"\\n--- Kernel finished. Back to CPU. ---\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618223e8",
   "metadata": {},
   "source": [
    "### Compile and Run\n",
    "\n",
    "Now we compile the code with `nvcc` and run the resulting executable (`./hello_gpu`) right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f889c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the .cu file; -o specifies the output executable name\n",
    "!nvcc -o hello_cuda hello_cuda.cu\n",
    "\n",
    "# Run the executable\n",
    "!./hello_cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b47c0",
   "metadata": {},
   "source": [
    "> ‚ö™Ô∏è **REFLECTION: Why is the output jumbled?**\n",
    "> You likely saw the \"Hello...\" messages printed in a random, jumbled order. **This is parallelism in action!** \n",
    "> \n",
    "> All 16 threads (2 blocks * 8 threads/block) ran at *roughly* the same time. They all tried to print to the screen (a single resource) at once, so the output order is not guaranteed. This is a fundamental concept of parallel programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23ccb9",
   "metadata": {},
   "source": [
    "## 4. A Real Task: Vector Addition\n",
    "\n",
    "\"Hello World\" is fun, but not useful. A classic parallel task is vector addition: `C[i] = A[i] + B[i]`.\n",
    "\n",
    "The logic is simple: **map one thread to each element `i` of the array.**\n",
    "\n",
    "This requires a new concept: **Host (CPU) memory vs. Device (GPU) memory.**\n",
    "\n",
    "1.  Your data (vectors A, B, C) starts on the Host.\n",
    "2.  You must **allocate** memory on the Device (GPU) with `cudaMalloc()`.\n",
    "3.  You must **copy** data from Host to Device with `cudaMemcpy()`.\n",
    "4.  You **launch the kernel** to compute `C = A + B` on the Device.\n",
    "5.  You **copy** the result (vector C) from Device back to Host.\n",
    "6.  You **free** the Device memory with `cudaFree()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vector_add.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// Helper function to check for CUDA errors\n",
    "void check(cudaError_t err, const char* msg) {\n",
    "    if (err != cudaSuccess) {\n",
    "        fprintf(stderr, \"CUDA Error: %s: %s\\n\", msg, cudaGetErrorString(err));\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "/* \n",
    " * Kernel for element-wise vector addition\n",
    " * C[i] = A[i] + B[i]\n",
    " */\n",
    "__global__ void vecAdd(float* A, float* B, float* C, int N) {\n",
    "    // Get this thread's global index\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // IMPORTANT: Add a \"bounds check\"\n",
    "    // We might launch more threads than N if N isn't a perfect multiple \n",
    "    // of our block size. This check prevents threads from writing \n",
    "    // out of bounds.\n",
    "    if (i < N) {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1024 * 1024; // Let's use a bigger vector: 1M elements\n",
    "    size_t size = N * sizeof(float); // Total bytes\n",
    "\n",
    "    // 1. Allocate memory on the Host (CPU)\n",
    "    float* h_A = (float*)malloc(size);\n",
    "    float* h_B = (float*)malloc(size);\n",
    "    float* h_C = (float*)malloc(size);\n",
    "\n",
    "    // Initialize Host data\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_A[i] = 1.0f;\n",
    "        h_B[i] = 2.0f;\n",
    "    }\n",
    "\n",
    "    // 2. Allocate memory on the Device (GPU)\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    check(cudaMalloc(&d_A, size), \"cudaMalloc A\");\n",
    "    check(cudaMalloc(&d_B, size), \"cudaMalloc B\");\n",
    "    check(cudaMalloc(&d_C, size), \"cudaMalloc C\");\n",
    "\n",
    "    // 3. Copy data from Host to Device\n",
    "    check(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice), \"Memcpy H2D A\");\n",
    "    check(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice), \"Memcpy H2D B\");\n",
    "\n",
    "    // 4. Set up Grid/Block dimensions and launch Kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    // This calculation finds the number of blocks needed to cover all N elements\n",
    "    // (N + threadsPerBlock - 1) / threadsPerBlock is a standard integer \n",
    "    // arithmetic trick for ceiling(N / threadsPerBlock)\n",
    "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
    "\n",
    "    printf(\"Launching kernel with %d blocks and %d threads per block...\\n\", \n",
    "           blocksPerGrid, threadsPerBlock);\n",
    "\n",
    "    vecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "    check(cudaGetLastError(), \"Kernel launch\");\n",
    "\n",
    "    // Wait for kernel to finish\n",
    "    check(cudaDeviceSynchronize(), \"Device synchronize\");\n",
    "\n",
    "    // 5. Copy result from Device back to Host\n",
    "    check(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost), \"Memcpy D2H C\");\n",
    "\n",
    "    // 6. Verify the result on the Host\n",
    "    bool success = true;\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        if (h_C[i] != 3.0f) {\n",
    "            printf(\"Verification FAILED at index %d! Got %f, expected 3.0\\n\", i, h_C[i]);\n",
    "            success = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    if (success) {\n",
    "        printf(\"Verification PASSED!\\n\");\n",
    "    }\n",
    "\n",
    "    // 7. Free Device and Host memory\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202d4fe",
   "metadata": {},
   "source": [
    "### Compile and Run Vector Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8edb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o vector_add_gpu vector_add.cu\n",
    "!./vector_add_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef75cf",
   "metadata": {},
   "source": [
    "> üü° **TASK: Experiment!**\n",
    "> 1.  Go back to the `%%writefile vector_add.cu` cell.\n",
    "> 2.  Change `int N = 1024 * 1024;` to `int N = 1024 * 1024 * 10;` (10 million elements).\n",
    "> 3.  Rerun the `%%writefile` cell to save the file.\n",
    "> 4.  Rerun the compile and run cell above.\n",
    "> \n",
    "> Did it still work? Did it take noticeably longer? You just processed 10 million elements in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d80697",
   "metadata": {},
   "source": [
    "## üêç 5. The Python Way: Numba and CuPy\n",
    "\n",
    "Writing, compiling, and running C++ in a notebook is clunky. For prototyping and in Python-based workflows, we can access CUDA directly from Python.\n",
    "\n",
    "* **CuPy:** Provides a `numpy`-like array interface (e.g., `cupy.array()`) that lives *on the GPU*. Operations like `A + B` are automatically run on the GPU.\n",
    "* **Numba:** A just-in-time (JIT) compiler that can turn Python functions into CUDA kernels using a decorator (`@cuda.jit`).\n",
    "\n",
    "Let's look at the *same* vector add kernel, but written in Python for Numba. Notice how similar it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a710a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def vecAdd_numba_kernel(A, B, C, N):\n",
    "    # Numba provides an easy way to get the global thread ID\n",
    "    i = cuda.grid(1) \n",
    "    \n",
    "    # The same bounds check is still needed!\n",
    "    if i < N:\n",
    "        C[i] = A[i] + B[i]\n",
    "\n",
    "# --- Main program (now in Python) ---\n",
    "\n",
    "N = 1024 * 1024 # 1M elements\n",
    "\n",
    "# 1. Create Host arrays (using numpy)\n",
    "h_A = np.ones(N, dtype=np.float32)\n",
    "h_B = np.full(N, 2.0, dtype=np.float32)\n",
    "h_C = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "# 2 & 3. Allocate AND Copy data from Host to Device\n",
    "# Numba's cuda.to_device() handles both steps\n",
    "d_A = cuda.to_device(h_A)\n",
    "d_B = cuda.to_device(h_B)\n",
    "d_C = cuda.to_device(h_C) # Allocate space for the result\n",
    "\n",
    "# 4. Set up Grid/Block and launch kernel\n",
    "threadsPerBlock = 256\n",
    "blocksPerGrid = (N + threadsPerBlock - 1) // threadsPerBlock\n",
    "\n",
    "print(f\"Launching Numba kernel with {blocksPerGrid} blocks and {threadsPerBlock} threads...\")\n",
    "\n",
    "vecAdd_numba_kernel[blocksPerGrid, threadsPerBlock](d_A, d_B, d_C, N)\n",
    "\n",
    "# 5. Copy result from Device back to Host\n",
    "# d_C.copy_to_host() handles this\n",
    "h_C_result = d_C.copy_to_host()\n",
    "\n",
    "# 6. Verify\n",
    "if np.all(h_C_result == 3.0):\n",
    "    print(\"Numba verification PASSED!\")\n",
    "else:\n",
    "    print(\"Numba verification FAILED!\")\n",
    "\n",
    "# 7. No manual free needed! Python's garbage collector handles it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af6215",
   "metadata": {},
   "source": [
    "## 6. Recap & Next Steps\n",
    "\n",
    "Congratulations! You have successfully written, compiled, and run code on a Perlmutter GPU using both raw CUDA C++ and Numba.\n",
    "\n",
    "> ‚ö™Ô∏è **Key Takeaways**\n",
    "> \n",
    "> * The **Host (CPU)** launches **Kernels** on the **Device (GPU)**.\n",
    "> * A kernel is run by a **Grid** of **Blocks**, which are made of **Threads**.\n",
    "> * The `blockIdx`, `threadIdx`, and `blockDim` variables are used to calculate a thread's unique global ID.\n",
    "> * You must manage memory explicitly: `cudaMalloc`, `cudaMemcpy`, `cudaFree`.\n",
    "> * Tools like Numba and CuPy provide a Python-friendly way to do the same thing.\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "* **NERSC CUDA Docs:** [NERSC Perlmutter CUDA Documentation](https://docs.nersc.gov/development/programming-models/cuda/)\n",
    "* **NVIDIA's Full Guide:** [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) (The official bible)\n",
    "* **Numba CUDA:** [CUDA Target for Numba on Github](https://github.com/NVIDIA/numba-cuda)\n",
    "\n",
    "--- \n",
    "\n",
    "### üü¢ Next Up\n",
    "\n",
    "In **Part 2**, we will explore how to make our kernels *fast* by using **Shared Memory** to optimize a matrix multiplication kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80b85b-f9d5-4985-a2cf-73e6bb406035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
