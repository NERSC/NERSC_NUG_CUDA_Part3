{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd1e8cf",
   "metadata": {},
   "source": [
    "![NERSC Logo](NERSClogo087.png)\n",
    "\n",
    "# Part 3 Notebook 2: Introduction CUDA Kernel Optimization with Tiling & Shared Memory\n",
    "\n",
    "Welcome to the Notebook 2 of our CUDA training! In Notebook 1, we learned how to write, compile, and run basic CUDA kernels. We successfully parallelized vector addition, but we used a *naive* approach for matrix multiplication.\n",
    "\n",
    "**Goal:** Understand *why* our first matrix multiplication kernel was slow and how to fix it using one of the most important CUDA optimization techniques: **tiling with shared memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be730fac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "* **Identify** Global Memory as a performance bottleneck.\n",
    "* **Explain** the concept of Shared Memory and its role in data reuse.\n",
    "* **Implement** a *tiled* matrix multiplication kernel.\n",
    "* **Use** `__syncthreads()` to manage thread block synchronization.\n",
    "* **Measure** and **compare** the performance of a naive vs. an optimized kernel.\n",
    "* **Demonstrate** *why* synchronization is critical by running a \"broken\" kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be61ab",
   "metadata": {},
   "source": [
    "## 1. Setup & Baseline: The Global Memory Bottleneck\n",
    "\n",
    "First, let's set up our environment just like in Part 1. We also need a baseline. The following code is a *naive* matrix multiplication kernel (`C = A * B`).\n",
    "\n",
    "Every thread computes one element of `C`. To do this, it must read one entire row of `A` and one entire column of `B`. All these reads go directly to **Global Memory (DRAM)**, which is *very slow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1469c-4f6d-41a0-95b6-3c06ba5c7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile baseline_matmul.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// Helper function to check for CUDA errors\n",
    "void check(cudaError_t err, const char* msg) {\n",
    "    if (err != cudaSuccess) {\n",
    "        fprintf(stderr, \"CUDA Error: %s: %s\\n\", msg, cudaGetErrorString(err));\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "/* \n",
    " * Naive Kernel: C[row, col] = A[row, :] * B[:, col]\n",
    " * Each thread reads N*2 elements from slow Global Memory.\n",
    " */\n",
    "__global__ void matmul_global(float* A, float* B, float* C, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < N && col < N) {\n",
    "        float val = 0.0f;\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            val += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1024;\n",
    "    size_t size = N * N * sizeof(float);\n",
    "    printf(\"[Global Kernel] Matrix size: %d x %d\\n\", N, N);\n",
    "\n",
    "    // Allocate Host memory\n",
    "    float* h_A = (float*)malloc(size);\n",
    "    float* h_B = (float*)malloc(size);\n",
    "    float* h_C = (float*)malloc(size);\n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        h_A[i] = 1.0f;\n",
    "        h_B[i] = 2.0f;\n",
    "    }\n",
    "\n",
    "    // Allocate Device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    check(cudaMalloc(&d_A, size), \"cudaMalloc A\");\n",
    "    check(cudaMalloc(&d_B, size), \"cudaMalloc B\");\n",
    "    check(cudaMalloc(&d_C, size), \"cudaMalloc C\");\n",
    "\n",
    "    // Copy to Device\n",
    "    check(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice), \"Memcpy A\");\n",
    "    check(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice), \"Memcpy B\");\n",
    "\n",
    "    // --- START: CUDA Event Timing --- \n",
    "    cudaEvent_t start, stop;\n",
    "    check(cudaEventCreate(&start), \"EventCreate start\");\n",
    "    check(cudaEventCreate(&stop), \"EventCreate stop\");\n",
    "    // --- END: CUDA Event Timing --- \n",
    "\n",
    "    // Launch Kernel\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGrid((N + 15) / 16, (N + 15) / 16);\n",
    "\n",
    "    check(cudaEventRecord(start), \"EventRecord start\");\n",
    "    matmul_global<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "    check(cudaEventRecord(stop), \"EventRecord stop\");\n",
    "\n",
    "    // Synchronize host to wait for the kernel to finish\n",
    "    check(cudaEventSynchronize(stop), \"EventSynchronize\");\n",
    "\n",
    "    float time_ms = 0;\n",
    "    check(cudaEventElapsedTime(&time_ms, start, stop), \"EventElapsedTime\");\n",
    "    printf(\"GLOBAL KERNEL TIME: %.3f ms\\n\", time_ms);\n",
    "\n",
    "    // Free memory\n",
    "    check(cudaEventDestroy(start), \"EventDestroy start\");\n",
    "    check(cudaEventDestroy(stop), \"EventDestroy stop\");\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ce906",
   "metadata": {},
   "outputs": [],
   "source": [
    "## these modules will load the cudatoolkit on perlmutter\n",
    "!module load cudatoolkit\n",
    "!nvidia-smi\n",
    "\n",
    "!nvcc -o baseline_matmul baseline_matmul.cu\n",
    "!./baseline_matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89394f0a",
   "metadata": {},
   "source": [
    "## 2. Tiling & Shared Memory\n",
    "\n",
    "That was slow! For N=1024, each thread read **2048** values from global memory. The total global memory traffic was massive.\n",
    "\n",
    "**Observation:** When computing a `C` tile, threads in a block access the *same* rows of `A` and columns of `B` repeatedly. We are re-fetching the same data from slow DRAM over and over.\n",
    "\n",
    "**The Solution: Tiling**\n",
    "\n",
    "1.  **Shared Memory:** Each GPU Streaming Multiprocessor (SM) has a small, *very fast* on-chip memory called **Shared Memory**. It's visible to *all threads in that block*.\n",
    "2.  **Tiling Strategy:** Break the large `A`, `B`, and `C` matrices into small tiles (e.g., 16x16).\n",
    "3.  **Cooperative Loading:** Have all threads in a block *cooperate* to load one tile of `A` and one tile of `B` from slow **Global Memory** into fast **Shared Memory**. This is done *once* per tile.\n",
    "4.  **Synchronize:** Use `__syncthreads()` to create a barrier. This forces all threads to *wait* until the shared memory is fully loaded before proceeding.\n",
    "5.  **Compute from Shared:** Have each thread compute its part of the `C` tile, but this time reading *only* from fast Shared Memory.\n",
    "6.  **Loop:** Repeat for all tiles needed to compute the final `C` element.\n",
    "\n",
    "This strategy maximizes **data reuse**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44973ae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Visual:** Imagine the 16x16 block of threads. \n",
    "1. They all work together to grab a 16x16 tile of A and a 16x16 tile of B from global memory and place them in their shared \"scratchpad\".\n",
    "2. They wait for everyone to finish (`__syncthreads()`).\n",
    "3. They compute a 16x16 piece of C, reading only from the fast scratchpad.\n",
    "4. They move to the next set of tiles, add the result, and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e71b7c",
   "metadata": {},
   "source": [
    "## 3. Task: Tiled Kernel Implementation\n",
    "\n",
    "Below is the code for the tiled kernel. Read the comments carefully, especially around `__shared__` and `__syncthreads()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tiled_matmul.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "// We have removed <helper_timer.h>\n",
    "\n",
    "#define TILE_DIM 16 // Tile dimension\n",
    "\n",
    "// Helper function (same as before)\n",
    "void check(cudaError_t err, const char* msg) {\n",
    "    if (err != cudaSuccess) {\n",
    "        fprintf(stderr, \"CUDA Error: %s: %s\\n\", msg, cudaGetErrorString(err));\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "/* \n",
    " * Tiled Kernel: C[row, col] = A[row, :] * B[:, col]\n",
    " * Optimized using shared memory.\n",
    " */\n",
    "__global__ void matmul_tiled(float* A, float* B, float* C, int N) {\n",
    "    \n",
    "    // 1. Declare Shared Memory tiles\n",
    "    __shared__ float A_tile[TILE_DIM][TILE_DIM];\n",
    "    __shared__ float B_tile[TILE_DIM][TILE_DIM];\n",
    "\n",
    "    // 2. Identify thread position\n",
    "    int tx = threadIdx.x; // Thread's col within the tile\n",
    "    int ty = threadIdx.y; // Thread's row within the tile\n",
    "    int col = blockIdx.x * TILE_DIM + tx; // Global col index\n",
    "    int row = blockIdx.y * TILE_DIM + ty; // Global row index\n",
    "\n",
    "    // 3. Loop over tiles to compute one C element\n",
    "    float val = 0.0f;\n",
    "    // Robust loop boundary for non-perfect multiples\n",
    "    for (int tile = 0; tile < (N + TILE_DIM - 1) / TILE_DIM; tile++) {\n",
    "        \n",
    "        // 4. Load data from Global to Shared Memory\n",
    "        int A_col = tile * TILE_DIM + tx;\n",
    "        int B_row = tile * TILE_DIM + ty;\n",
    "\n",
    "        if (row < N && A_col < N) {\n",
    "            A_tile[ty][tx] = A[row * N + A_col];\n",
    "        } else {\n",
    "            A_tile[ty][tx] = 0.0f; // Padding\n",
    "        }\n",
    "        \n",
    "        if (B_row < N && col < N) {\n",
    "            B_tile[ty][tx] = B[B_row * N + col];\n",
    "        } else {\n",
    "            B_tile[ty][tx] = 0.0f; // Padding\n",
    "        }\n",
    "\n",
    "        // 5. SYNCHRONIZE!\n",
    "        __syncthreads();\n",
    "\n",
    "        // 6. Compute using fast Shared Memory\n",
    "        for (int k = 0; k < TILE_DIM; k++) {\n",
    "            val += A_tile[ty][k] * B_tile[k][tx];\n",
    "        }\n",
    "        \n",
    "        // 7. SYNCHRONIZE AGAIN!\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // 8. Write the final result to Global Memory\n",
    "    if (row < N && col < N) {\n",
    "        C[row * N + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1024;\n",
    "    size_t size = N * N * sizeof(float);\n",
    "    printf(\"[Tiled Kernel] Matrix size: %d x %d\\n\", N, N);\n",
    "\n",
    "    float* h_A = (float*)malloc(size);\n",
    "    float* h_B = (float*)malloc(size);\n",
    "    float* h_C = (float*)malloc(size);\n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        h_A[i] = 1.0f;\n",
    "        h_B[i] = 2.0f;\n",
    "    }\n",
    "\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    check(cudaMalloc(&d_A, size), \"cudaMalloc A\");\n",
    "    check(cudaMalloc(&d_B, size), \"cudaMalloc B\");\n",
    "    check(cudaMalloc(&d_C, size), \"cudaMalloc C\");\n",
    "\n",
    "    check(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice), \"Memcpy A\");\n",
    "    check(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice), \"Memcpy B\");\n",
    "\n",
    "    // --- START: CUDA Event Timing --- \n",
    "    cudaEvent_t start, stop;\n",
    "    check(cudaEventCreate(&start), \"EventCreate start\");\n",
    "    check(cudaEventCreate(&stop), \"EventCreate stop\");\n",
    "    // --- END: CUDA Event Timing --- \n",
    "\n",
    "    // Block and Grid dimensions are based on TILE_DIM\n",
    "    dim3 threadsPerBlock(TILE_DIM, TILE_DIM);\n",
    "    dim3 blocksPerGrid((N + TILE_DIM - 1) / TILE_DIM, (N + TILE_DIM - 1) / TILE_DIM);\n",
    "\n",
    "    check(cudaEventRecord(start), \"EventRecord start\");\n",
    "    matmul_tiled<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "    check(cudaEventRecord(stop), \"EventRecord stop\");\n",
    "\n",
    "    check(cudaEventSynchronize(stop), \"EventSynchronize\");\n",
    "\n",
    "    float time_ms = 0;\n",
    "    check(cudaEventElapsedTime(&time_ms, start, stop), \"EventElapsedTime\");\n",
    "    printf(\"TILED KERNEL TIME: %.3f ms\\n\", time_ms);\n",
    "\n",
    "    // Free memory\n",
    "    check(cudaEventDestroy(start), \"EventDestroy start\");\n",
    "    check(cudaEventDestroy(stop), \"EventDestroy stop\");\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f97e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o tiled_gpu tiled_matmul.cu \n",
    "!./tiled_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178bfd0",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "Look at the times! The tiled kernel should be *significantly* faster.\n",
    "\n",
    "> **REFLECTION**\n",
    "> \n",
    "> * What was your Global Kernel time?\n",
    "> * What was your Tiled Kernel time?\n",
    "> * What is the **speedup** (Global Time / Tiled Time)?\n",
    "> \n",
    "> You just saved a huge amount of time by reducing global memory traffic and maximizing data reuse in fast shared memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94380a",
   "metadata": {},
   "source": [
    "## 5. Synchronization Demo: What if we forget `__syncthreads()`?\n",
    "\n",
    "This is one of the most common and dangerous bugs in CUDA programming. What happens if we remove the `__syncthreads()` call?\n",
    "\n",
    "**Hypothesis:** If we remove the sync, threads will enter the compute loop (Step 6) *before* other threads have finished loading the data (Step 4). They will compute using *garbage data* or *stale data* from a previous tile. This is called a **race condition**.\n",
    "\n",
    "Let's try it. We'll add a verification step to `main()` to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile broken_matmul.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h> // For fabs()\n",
    "\n",
    "#define TILE_DIM 16\n",
    "\n",
    "void check(cudaError_t err, const char* msg) {\n",
    "    if (err != cudaSuccess) {\n",
    "        fprintf(stderr, \"CUDA Error: %s: %s\\n\", msg, cudaGetErrorString(err));\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void matmul_broken(float* A, float* B, float* C, int N) {\n",
    "    __shared__ float A_tile[TILE_DIM][TILE_DIM];\n",
    "    __shared__ float B_tile[TILE_DIM][TILE_DIM];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_DIM + tx;\n",
    "    int row = blockIdx.y * TILE_DIM + ty;\n",
    "\n",
    "    float val = 0.0f;\n",
    "    // Corrected loop boundary\n",
    "    for (int tile = 0; tile < (N + TILE_DIM - 1) / TILE_DIM; tile++) {\n",
    "        \n",
    "        // Load data from Global to Shared Memory\n",
    "        int A_col = tile * TILE_DIM + tx;\n",
    "        int B_row = tile * TILE_DIM + ty;\n",
    "\n",
    "        if (row < N && A_col < N) {\n",
    "            A_tile[ty][tx] = A[row * N + A_col];\n",
    "        } else {\n",
    "            A_tile[ty][tx] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        if (B_row < N && col < N) {\n",
    "            B_tile[ty][tx] = B[B_row * N + col];\n",
    "        } else {\n",
    "            B_tile[ty][tx] = 0.0f;\n",
    "        }\n",
    "\n",
    "        // 5. SYNCHRONIZE! - WE COMMENTED THIS OUT!\n",
    "        // __syncthreads();\n",
    "\n",
    "        // Compute using (garbage) Shared Memory\n",
    "        for (int k = 0; k < TILE_DIM; k++) {\n",
    "            val += A_tile[ty][k] * B_tile[k][tx];\n",
    "        }\n",
    "        \n",
    "        // 7. AND WE COMMENTED OUT THIS ONE TOO!\n",
    "        // __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (row < N && col < N) {\n",
    "        C[row * N + col] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Main function now has verification\n",
    "int main() {\n",
    "    int N = 1024;\n",
    "    size_t size = N * N * sizeof(float);\n",
    "    printf(\"[Broken Kernel] Matrix size: %d x %d\\n\", N, N);\n",
    "\n",
    "    float* h_A = (float*)malloc(size);\n",
    "    float* h_B = (float*)malloc(size);\n",
    "    float* h_C = (float*)malloc(size);\n",
    "    for (int i = 0; i < N*N; i++) {\n",
    "        h_A[i] = 1.0f;\n",
    "        h_B[i] = 2.0f;\n",
    "    }\n",
    "\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    check(cudaMalloc(&d_A, size), \"cudaMalloc A\");\n",
    "    check(cudaMalloc(&d_B, size), \"cudaMalloc B\");\n",
    "    check(cudaMalloc(&d_C, size), \"cudaMalloc C\");\n",
    "    check(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice), \"Memcpy A\");\n",
    "    check(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice), \"Memcpy B\");\n",
    "\n",
    "    dim3 threadsPerBlock(TILE_DIM, TILE_DIM);\n",
    "    dim3 blocksPerGrid((N + TILE_DIM - 1) / TILE_DIM, (N + TILE_DIM - 1) / TILE_DIM);\n",
    "\n",
    "    printf(\"Launching BROKEN kernel...\\n\");\n",
    "    matmul_broken<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "    check(cudaDeviceSynchronize(), \"Kernel sync\");\n",
    "\n",
    "    // Copy result back and VERIFY\n",
    "    check(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost), \"Memcpy C\");\n",
    "\n",
    "    // A=1, B=2. C[i] should be sum(1.0 * 2.0) N times = 2.0 * N\n",
    "    float expected = 2.0f * N;\n",
    "    bool success = true;\n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        if (fabs(h_C[i] - expected) > 1e-3) {\n",
    "            printf(\"VERIFICATION FAILED at index %d! Got %.2f, expected %.2f\\n\", \n",
    "                   i, h_C[i], expected);\n",
    "            success = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    if (success) {\n",
    "        printf(\"VERIFICATION PASSED! (This is unlikely...)\\n\");\n",
    "    }\n",
    "\n",
    "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "    free(h_A); free(h_B); free(h_C);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o broken_matmul broken_matmul.cu\n",
    "\n",
    "!./broken_matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f230f",
   "metadata": {},
   "source": [
    "> 🔹 **INFO: VERIFICATION FAILED!**\n",
    "> \n",
    "> You should see a \"VERIFICATION FAILED\" message. The output values are garbage. This demonstrates that `__syncthreads()` is not optional—it is **absolutely essential** for correctness when threads in a block share data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e0380",
   "metadata": {},
   "source": [
    "## 6. Recap & Next Steps\n",
    "\n",
    "> **Key Takeaways**\n",
    "> \n",
    "> * **Global Memory is slow.** Accessing it is a key performance bottleneck.\n",
    "> * **Shared Memory is fast.** It's a programmable, on-chip cache for a thread block.\n",
    "> * **Tiling** is a strategy to load data from global to shared memory, compute, and maximize data reuse.\n",
    "> * **`__syncthreads()`** is a barrier that forces all threads in a block to wait. It is **critical** for correctness when using shared memory to prevent race conditions.\n",
    "\n",
    "You now have the fundamental building blocks for writing high-performance CUDA code!\n",
    "\n",
    "### 📚 Resources & Further Reading\n",
    "\n",
    "* **NERSC GPU Docs:** [NERSC Perlmutter GPU Reference](https://docs.nersc.gov/systems/perlmutter/architecture/#gpus)\n",
    "* **NVIDIA CUDA Best Practices:** [CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) (See the \"Shared Memory\" section)\n",
    "* **NERSC Slack & Training:** Ask questions in the `#nersc-users` Slack channel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3641a7-3872-4ac9-ba97-d9528c5fef1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
